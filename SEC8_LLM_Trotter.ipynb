{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d6f286-3aff-4afc-9776-4d9fab3bcb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"MY API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6761c4-5b6f-4c82-afbf-85891c2a9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d099558c-413c-4f56-b860-48f2587a90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\": \"Rafael Trotter ra494491@ucf.edu\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2154b707-9302-4e57-9647-907dbf7d33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"8K_Filings\"\n",
    "OUTPUT_CSV = \"8-k_newProducts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac58fe4-6194-490d-8748-88a1962b8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1eb7741-a837-4f84-a9c3-ade5db654a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing: AAPL_8K_10_2024-05-02.txt\n",
      " Processing: AAPL_8K_1_2025-02-25.txt\n",
      " Processing: AAPL_8K_2_2025-01-30.txt\n",
      " Processing: AAPL_8K_3_2025-01-03.txt\n",
      " Processing: AAPL_8K_4_2024-10-31.txt\n",
      " Processing: AAPL_8K_5_2024-09-10.txt\n",
      " Processing: AAPL_8K_6_2024-08-26.txt\n",
      " Processing: AAPL_8K_7_2024-08-23.txt\n",
      " Processing: AAPL_8K_8_2024-08-01.txt\n",
      " Processing: AAPL_8K_9_2024-05-03.txt\n",
      " Processing: AMZN_8K_10_2023-11-01.txt\n",
      " Processing: AMZN_8K_1_2025-02-06.txt\n",
      " Processing: AMZN_8K_2_2024-10-31.txt\n",
      " Error processing AMZN_8K_2_2024-10-31.txt: Extra data: line 4 column 2 (char 103)\n",
      " Processing: AMZN_8K_3_2024-08-01.txt\n",
      " Processing: AMZN_8K_4_2024-05-24.txt\n",
      " Processing: AMZN_8K_5_2024-05-14.txt\n",
      " Error processing AMZN_8K_5_2024-05-14.txt: The markup you provided was rejected by the parser. Trying a different parser or a different encoding may help.\n",
      "\n",
      "Original exception(s) from parser:\n",
      " AssertionError: expected name token at '<![(V#1(2\\\\Y32.RXI/Y('\n",
      " Processing: AMZN_8K_6_2024-05-03.txt\n",
      " Processing: AMZN_8K_7_2024-04-30.txt\n",
      " Processing: AMZN_8K_8_2024-04-10.txt\n",
      " Processing: AMZN_8K_9_2024-02-01.txt\n",
      " Processing: GOOGL_8K_10_2024-06-07.txt\n",
      " Processing: GOOGL_8K_1_2025-03-18.txt\n",
      " Processing: GOOGL_8K_2_2025-02-04.txt\n",
      " Processing: GOOGL_8K_3_2024-10-29.txt\n",
      " Processing: GOOGL_8K_4_2024-10-17.txt\n",
      " Processing: GOOGL_8K_5_2024-09-24.txt\n",
      " Processing: GOOGL_8K_6_2024-08-05.txt\n",
      " Processing: GOOGL_8K_7_2024-07-23.txt\n",
      " Processing: GOOGL_8K_8_2024-06-26.txt\n",
      " Error processing GOOGL_8K_8_2024-06-26.txt: The markup you provided was rejected by the parser. Trying a different parser or a different encoding may help.\n",
      "\n",
      "Original exception(s) from parser:\n",
      " AssertionError: expected name token at '<![$\"9,\"\\nMAGS&\\\\JXX7T'\n",
      " Processing: GOOGL_8K_9_2024-06-13.txt\n",
      " Processing: JPM_8K_10_2024-10-22.txt\n",
      " Processing: JPM_8K_1_2025-02-04.txt\n",
      " Processing: JPM_8K_2_2025-01-27.txt\n",
      " Processing: JPM_8K_3_2025-01-24.txt\n",
      " Processing: JPM_8K_4_2025-01-23.txt\n",
      " Processing: JPM_8K_5_2025-01-15.txt\n",
      " Processing: JPM_8K_6_2025-01-15.txt\n",
      " Processing: JPM_8K_7_2025-01-14.txt\n",
      " Processing: JPM_8K_8_2024-12-12.txt\n",
      " Processing: JPM_8K_9_2024-11-29.txt\n",
      " Processing: META_8K_10_2024-02-14.txt\n",
      " Processing: META_8K_1_2025-02-20.txt\n",
      " Processing: META_8K_2_2025-01-29.txt\n",
      " Processing: META_8K_3_2025-01-06.txt\n",
      " Processing: META_8K_4_2024-10-30.txt\n",
      " Processing: META_8K_5_2024-09-10.txt\n",
      " Processing: META_8K_6_2024-08-09.txt\n",
      " Processing: META_8K_7_2024-07-31.txt\n",
      " Processing: META_8K_8_2024-05-31.txt\n",
      " Processing: META_8K_9_2024-04-24.txt\n",
      " Processing: MSFT_8K_10_2024-01-30.txt\n",
      " Processing: MSFT_8K_1_2025-01-29.txt\n",
      " Processing: MSFT_8K_2_2025-01-22.txt\n",
      " Processing: MSFT_8K_3_2024-12-11.txt\n",
      " Processing: MSFT_8K_4_2024-12-03.txt\n",
      " Processing: MSFT_8K_5_2024-10-30.txt\n",
      " Processing: MSFT_8K_6_2024-08-21.txt\n",
      " Processing: MSFT_8K_7_2024-07-30.txt\n",
      " Processing: MSFT_8K_8_2024-04-25.txt\n",
      " Processing: MSFT_8K_9_2024-03-08.txt\n",
      " Processing: NVDA_8K_10_2024-03-14.txt\n",
      " Error processing NVDA_8K_10_2024-03-14.txt: The markup you provided was rejected by the parser. Trying a different parser or a different encoding may help.\n",
      "\n",
      "Original exception(s) from parser:\n",
      " AssertionError: expected name token at \"<![@7 KQC@%=P'VY44$Z\"\n",
      " Processing: NVDA_8K_1_2025-03-07.txt\n",
      " Processing: NVDA_8K_2_2025-02-26.txt\n",
      " Processing: NVDA_8K_3_2025-01-17.txt\n",
      " Processing: NVDA_8K_4_2024-11-20.txt\n",
      " Processing: NVDA_8K_5_2024-11-07.txt\n",
      " Processing: NVDA_8K_6_2024-08-28.txt\n",
      " Processing: NVDA_8K_7_2024-07-02.txt\n",
      " Processing: NVDA_8K_8_2024-06-07.txt\n",
      " Processing: NVDA_8K_9_2024-05-22.txt\n",
      " Processing: PEP_8K_10_2024-05-03.txt\n",
      " Processing: PEP_8K_1_2025-02-07.txt\n",
      " Processing: PEP_8K_2_2025-02-07.txt\n",
      " Processing: PEP_8K_3_2025-02-03.txt\n",
      " Processing: PEP_8K_4_2024-12-11.txt\n",
      " Processing: PEP_8K_5_2024-10-07.txt\n",
      " Processing: PEP_8K_6_2024-09-20.txt\n",
      " Processing: PEP_8K_7_2024-07-17.txt\n",
      " Processing: PEP_8K_8_2024-07-10.txt\n",
      " Processing: PEP_8K_9_2024-05-24.txt\n",
      " Processing: PFE_8K_10_2024-05-01.txt\n",
      " Processing: PFE_8K_1_2025-02-04.txt\n",
      " Processing: PFE_8K_2_2024-12-17.txt\n",
      " Processing: PFE_8K_3_2024-10-29.txt\n",
      " Processing: PFE_8K_4_2024-10-15.txt\n",
      " Processing: PFE_8K_5_2024-07-30.txt\n",
      " Processing: PFE_8K_6_2024-07-26.txt\n",
      " Processing: PFE_8K_7_2024-07-10.txt\n",
      " Processing: PFE_8K_8_2024-07-01.txt\n",
      " Processing: PFE_8K_9_2024-05-22.txt\n",
      " Processing: TSLA_8K_10_2024-04-02.txt\n",
      " Processing: TSLA_8K_1_2025-01-29.txt\n",
      " Processing: TSLA_8K_2_2025-01-02.txt\n",
      " Processing: TSLA_8K_3_2024-10-23.txt\n",
      " Processing: TSLA_8K_4_2024-10-02.txt\n",
      " Processing: TSLA_8K_5_2024-07-23.txt\n",
      " Processing: TSLA_8K_6_2024-07-02.txt\n",
      " Processing: TSLA_8K_7_2024-06-14.txt\n",
      " Processing: TSLA_8K_8_2024-04-23.txt\n",
      " Processing: TSLA_8K_9_2024-04-16.txt\n",
      "\n",
      " Done! Processed 100 filings. Saved to 8-k_newProducts.csv.\n"
     ]
    }
   ],
   "source": [
    "# Acquire ticker â†’ name/cik mapping\n",
    "def load_ticker_data():\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    return {\n",
    "        v[\"ticker\"].upper(): {\n",
    "            \"cik\": str(v[\"cik_str\"]).zfill(10),\n",
    "            \"title\": v[\"title\"]\n",
    "        } for v in response.json().values()\n",
    "    }\n",
    "\n",
    "# Store ticker data \n",
    "TICKER_DATA = load_ticker_data()\n",
    "\n",
    "# Extract clean, relevant text from 8-K file\n",
    "def extract_clean_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_html = f.read()\n",
    "\n",
    "    # starting marker\n",
    "    start_marker = \"Copyright 2024 Workiva\"\n",
    "    if start_marker in raw_html:\n",
    "        raw_html = raw_html.split(start_marker, 1)[-1]\n",
    "\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "# Activate LLM\n",
    "def extract_product_info(text):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in SEC filings. Analyze, and search the following 8-K text and identify any new product announcements.\n",
    "\n",
    "Return ONLY JSON in this format:\n",
    "{{\n",
    "  \"new_product\": \"Product Name\",\n",
    "  \"product_description\": \"Brief explanation of the product\"\n",
    "}}\n",
    "\n",
    "If no new product is mentioned, return:\n",
    "{{\n",
    "  \"new_product\": null,\n",
    "  \"product_description\": null\n",
    "}}\n",
    "\n",
    "Here is the 8-K text:\n",
    "{text[:10000]}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# look for ticker and date from filename\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    stock = parts[0]\n",
    "    date = parts[-1].replace(\".txt\", \"\")\n",
    "    return stock.upper(), date\n",
    "\n",
    "# Main \n",
    "def process_all_files():\n",
    "    results = []\n",
    "    txt_files = sorted([file for file in os.listdir(INPUT_DIR) if file.endswith(\".txt\")])[:]\n",
    "\n",
    "    for file in txt_files:\n",
    "        file_path = os.path.join(INPUT_DIR, file)\n",
    "        print(f\" Processing: {file}\")\n",
    "\n",
    "        try:\n",
    "            clean_text = extract_clean_text(file_path)\n",
    "            json_text = extract_product_info(clean_text)\n",
    "            response_json = json.loads(json_text)\n",
    "\n",
    "            stock, filing_date = parse_filename(file)\n",
    "            company_info = TICKER_DATA.get(stock, {\"title\": \"Unknown\", \"cik\": \"N/A\"})\n",
    "\n",
    "            new_product = response_json.get(\"new_product\")\n",
    "            product_description = response_json.get(\"product_description\")\n",
    "\n",
    "            results.append({\n",
    "                \"company_name\": company_info[\"title\"],\n",
    "                \"stock_name\": stock,\n",
    "                \"filing_time\": filing_date,\n",
    "                \"new_product\": new_product if new_product else \"not mentioned\",\n",
    "                \"product_description\": product_description if product_description else \"not mentioned\"\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {file}: {e}\")\n",
    "            stock, filing_date = parse_filename(file)\n",
    "            company_info = TICKER_DATA.get(stock, {\"title\": \"Unknown\", \"cik\": \"N/A\"})\n",
    "            results.append({\n",
    "                \"company_name\": company_info[\"title\"],\n",
    "                \"stock_name\": stock,\n",
    "                \"filing_time\": filing_date,\n",
    "                \"new_product\": \"not mentioned\",\n",
    "                \"product_description\": \"not mentioned\"\n",
    "            })\n",
    "\n",
    "    # Save output to CSV\n",
    "    with open(OUTPUT_CSV, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        fieldnames = [\"company_name\", \"stock_name\", \"filing_time\", \"new_product\", \"product_description\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"\\n Done! Processed {len(results)} filings. Saved to {OUTPUT_CSV}.\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d508767-a9db-4c2d-8a3b-888ddb4af6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
